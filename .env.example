# -----------------------------------------------------------------------------
# ðŸ¤– AI PROVIDER KEYS
# -----------------------------------------------------------------------------
# Get your keys at: 
# https://console.anthropic.com/
# https://platform.openai.com/
ANTHROPIC_API_KEY=sk-ant-your-key-here
OPENAI_API_KEY=sk-your-key-here
DEEPSEEK_API_KEY=your-key-here
GEMINI_API_KEY=your-gemini-key-here

# Default OpenClaw model (recommended cheaper default).
# Examples: google/gemini-2.5-flash, openai/gpt-4.1-mini
OPENCLAW_DEFAULT_MODEL=google/gemini-2.5-flash

# Gateway auth token for OpenClaw WebSocket/API access.
# Use a strong random value (example): openssl rand -hex 32
# If left empty, OpenClaw may auto-generate one in container state, but explicit
# config is recommended for predictable, secure access.
OPENCLAW_GATEWAY_TOKEN=
# Optional alternative to token auth:
# OPENCLAW_GATEWAY_PASSWORD=
#
# Backward-compat alias (used only if OPENCLAW_GATEWAY_TOKEN is empty):
# GATEWAY_TOKEN=

# -----------------------------------------------------------------------------
# ðŸ›¡ï¸ SANDBOX & WORKSPACE CONFIG
# -----------------------------------------------------------------------------
# The local path to the code you want the agent to work on.
# Example: /Users/smail/projects/industrial-system-v1
LOCAL_WORKSPACE_PATH=./workspace

# -----------------------------------------------------------------------------
# ðŸš¥ GATEWAY & PROXY SETTINGS
# -----------------------------------------------------------------------------
# Do not set host-level HTTP(S)_PROXY here.
# Docker service proxy routing is configured directly in docker-compose.yml.

# -----------------------------------------------------------------------------
# ðŸ” SECURITY INSPECTOR (SHIELD API)
# -----------------------------------------------------------------------------
# The URL where the agent sends prompts for pre-scan (LlamaFirewall + Intent).
SCANNER_URL=http://inspector-api:5000/scan

# The local model used by the Shield API for intent classification.
SECURITY_MODEL=qwen2.5:0.5b
OLLAMA_HOST=http://ollama:11434

# Path to the shared HAProxy runtime socket (must match docker-compose).
HAPROXY_SOCKET=/var/run/haproxy.sock

# -----------------------------------------------------------------------------
# ðŸ§± EXEC GATEKEEPER (PRE-EXEC POLICY)
# -----------------------------------------------------------------------------
# observe: returns decision metadata but does not enforce blocking.
# enforce: enforces allow/deny/require_approval decisions.
EXEC_GUARD_MODE=observe

# Behavior when LLM/classifier fails or times out:
# approval -> require human approval
# closed   -> deny command
EXEC_GUARD_FAIL_MODE=approval

# Timeout budget for command risk classification.
EXEC_GUARD_TIMEOUT_MS=2500

# -----------------------------------------------------------------------------
# ðŸ“± TELEGRAM BOT CONFIG
# -----------------------------------------------------------------------------
# The token for the Telegram bot.
# Get your token at: https://t.me/botfather
TELEGRAM_BOT_TOKEN=your-token-here

# Comma-separated Telegram user IDs allowed to DM the bot.
# Example: TELEGRAM_ALLOWED_IDS=5042462888,123456789
TELEGRAM_ALLOWED_IDS=
